#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
@Project ：gyh 
@File    ：shadow_training.py
@Author  ：Gu Yuhao
@Date    ：2022/6/9 下午4:08 

Implementation of shadow datasets generating and shadow models training steps in LDIA
"""
import copy
import logging

import numpy as np
import torch
from numpy.random import shuffle
from torch.utils import data

from attack.ldia.data_augmentation import data_duplicate
from attack.utils import get_random_label_distribution, get_quantity_label_distribution
from common import train_model
from dataset.utils import get_dataset_labels
from federated.utils import fed_aggregation


def get_shadow_loaders(ori_loader, args, train_size_list=None, special_size=None):
    """
    generate shadow datasets

    :param ori_loader: loader of auxiliary set
    :param args: configuration
    :param train_size_list: users' data size
    :param special_size: number of shadow datasets generated by our method, default None means all
    :return: loaders of shadow datasets
    """
    # enlarge auxiliary data by duplication
    all_loader = data_duplicate(ori_loader, args)
    # get all indices of the enlarged auxiliary data
    all_dataset = all_loader.dataset
    labels = get_dataset_labels(all_dataset, args['dataset'])
    if isinstance(all_dataset, torch.utils.data.Subset):
        all_indices = np.array(all_dataset.indices)
        all_dataset = all_dataset.dataset
    else:
        all_indices = np.arange(len(all_dataset))

    size = args['ldia_shadow_number']
    client_data_size = args['client_train_size']
    class_indices = [all_indices[np.where(labels == i)[0]] for i in range(args['num_classes'])]
    client_indices = []

    # decide data size of each shadow dataset by resampling users' data size
    client_size_list = None
    if train_size_list is not None:
        times = size // len(train_size_list)
        remain = size % len(train_size_list)
        client_size_list = []
        for j in range(times):
            client_size_list += train_size_list
        if remain > 0:
            temp_indices = np.random.choice(np.arange(len(train_size_list)), remain, replace=False)
            for index in temp_indices:
                client_size_list.append(train_size_list[index])
        shuffle(client_size_list)

    # generate shadow datasets using our method
    n_class = args['num_classes']
    quantity_list = list(range(1, n_class, 1))
    if n_class not in quantity_list:
        quantity_list.append(n_class)
    per_special_size = size // len(quantity_list)
    if special_size is None:
        special_size = per_special_size * len(quantity_list)
    for i in range(special_size):
        temp_indices = None
        temp_distribution = get_quantity_label_distribution(
            client_size_list[i] if client_size_list is not None else client_data_size, args['num_classes'],
            quantity_list[i//per_special_size])
        for j, temp in enumerate(temp_distribution):
            shuffle(class_indices[j])
            if temp != 0:
                temp_indices = np.concatenate((temp_indices, class_indices[j][:temp])) \
                    if temp_indices is not None else class_indices[j][:temp]
        client_indices.append(temp_indices)
    size = size - special_size

    # generate remaining shadow datasets by random distribution, 0 in paper
    for i in range(size):
        temp_indices = None
        temp_distribution = get_random_label_distribution(
            client_size_list[i] if client_size_list is not None else client_data_size, args['num_classes'])
        for j, temp in enumerate(temp_distribution):
            shuffle(class_indices[j])
            if temp != 0:
                temp_indices = np.concatenate((temp_indices, class_indices[j][:temp])) \
                    if temp_indices is not None else class_indices[j][:temp]
        client_indices.append(temp_indices)

    # generate loaders of shadow datasets
    shadow_loaders = []
    for indices in client_indices:
        if isinstance(all_dataset, list):
            dataset = list([all_dataset[index] for index in indices])
            loader = data.DataLoader(dataset=dataset, batch_size=args['target_batch_size'], shuffle=True,
                                     collate_fn=ori_loader.collate_fn)
        else:
            dataset = torch.utils.data.Subset(all_dataset, indices)
            loader = data.DataLoader(dataset=dataset, batch_size=args['target_batch_size'], shuffle=True)
        shadow_loaders.append(loader)
    return shadow_loaders


def server_train(global_model, round, select_client, args, train_loader, shadow_index):
    """
    attacker perform shadow local model training

    :param global_model: pre-round shadow global model
    :param round: current round
    :param select_client: whether is in fractional aggregation
    :param args: configuration
    :param train_loader: loader of shadow dataset
    :param shadow_index: index of shadow model
    :return: tuple containing:
        (1) model: trained shaodw local model
        (2) size: size of shadow
    """
    model = copy.deepcopy(global_model)
    if select_client:
        model_type = 'shadow_select_{}_{}client_local_{}_{}'.format(
            args['target_model'], args['n_client'], shadow_index, round)
    else:
        model_type = 'shadow_all_{}_{}client_local_{}_{}'.format(
            args['target_model'], args['n_client'], shadow_index, round)

    # train shadow local model without defense
    train_model(model=model,
                model_type=model_type,
                train_loader=train_loader,
                test_loader=None,
                args=args,
                load=args['load_shadow'])

    # ignore parameters of the embedding layer on text datasets
    result_dict = model.state_dict()
    if args['dataset'] in ['ag_news', 'imdb']:
        del result_dict['embed.weight']

    return result_dict, len(train_loader.dataset)


class ShadowTrain:
    def __init__(self, shadow_loaders, init_model, select_client, args, ldia_attack, dp=False):
        self.shadow_loaders = shadow_loaders
        self.select_client = select_client
        self.args = args
        self.model_arch = copy.deepcopy(init_model)
        self.ldia_attack = ldia_attack
        self.train_x = torch.zeros((len(shadow_loaders),
                                    args['rounds'],
                                    self.ldia_attack.metric_split_indices[-1]))

        # initialize shadow global models
        for i in range(len(shadow_loaders)):
            self.save_global_model(copy.deepcopy(self.model_arch), i, 0)

        self.dp = dp

    def save_global_model(self, global_model, index, round):
        """
        save parameters of shadow global model into local file

        :param global_model: shadow global model
        :param index: index of the corresponding shadow dataset
        :param round: current round
        :return:
        """
        if self.args['save_model']:
            if self.select_client:
                model_type = 'shadow_select_{}_{}client_global_{}_{}'.format(
                    self.args['target_model'], self.args['n_client'], index, round)
            else:
                model_type = 'shadow_all_{}_{}client_global_{}_{}'.format(
                    self.args['target_model'], self.args['n_client'], index, round)
            global_model.save(name=model_type)

    def train_per_round(self, client_models, round):
        """
        perform shadow model training in each round

        :param client_models: local model parameters uploaded by users in current round
        :param round: current round
        :return:
        """
        logging.debug("Global Round {} Training Server".format(round))
        # train shadow local model for each shadow dataset
        for shadow_index, shadow_loader in enumerate(self.shadow_loaders):
            global_model = copy.deepcopy(self.model_arch)
            if self.select_client:
                global_model.load(name='shadow_select_{}_{}client_global_{}_{}'.format(
                    self.args['target_model'], self.args['n_client'], shadow_index, round))
            else:
                global_model.load(name='shadow_all_{}_{}client_global_{}_{}'.format(
                    self.args['target_model'], self.args['n_client'], shadow_index, round))

            # train shadow local model
            server_model, server_data_size = server_train(
                global_model=global_model,
                round=round,
                select_client=self.select_client,
                args=self.args,
                train_loader=shadow_loader,
                shadow_index=shadow_index
            )

            # aggregate to get shadow global model
            fake_client_models = list()
            fake_client_models += client_models
            fake_client_models.append({'index': -1, 'model': server_model, 'data_size': server_data_size})
            prev_global_model = copy.deepcopy(global_model)
            global_model = fed_aggregation(
                global_model=global_model,
                client_models=fake_client_models,
                args=self.args,
                dp=self.dp
            )

            if round != self.args['rounds']-1:
                self.save_global_model(global_model, shadow_index, round+1)
            # calculate output layer updates of shadow model in current round
            x = self.ldia_attack.get_feature_per_round(shadow_index, round, train=True)
            self.train_x[shadow_index:shadow_index + 1, round: round + 1, :] = x

            del server_model, global_model
        return
